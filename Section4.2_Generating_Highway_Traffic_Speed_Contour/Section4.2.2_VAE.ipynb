{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b993a52c-d386-413b-83ea-b2e963d23bf3",
   "metadata": {},
   "source": [
    "# Section 4.2.2. VAE Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d2d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import *\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf379b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Define the VAE model\n",
    "# -------------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim, input_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.enc_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_dim, 32, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.enc_fc_mu = nn.Linear(128 * 8 * 8, z_dim)\n",
    "        self.enc_fc_logvar = nn.Linear(128 * 8 * 8, z_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.dec_fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128 * 8 * 8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.dec_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, input_dim, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.enc_conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.enc_fc_mu(x)\n",
    "        logvar = self.enc_fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.dec_fc(z)\n",
    "        z = z.view(z.size(0), 128, 8, 8)\n",
    "        z = self.dec_conv(z)\n",
    "        return z\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca16b2-94e5-49ea-b199-748bf0b7a3f9",
   "metadata": {},
   "source": [
    "## 1. Training Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540d73b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Loss function\n",
    "# -------------------------------\n",
    "def loss_fn(recon_x, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    MSE based reconstruction loss and KL divergence loss for VAE.\n",
    "    KL divergence between the latent distribution and the standard normal distribution.\n",
    "    \"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def train_vae_model(model, dataloader, optimizer, num_epochs, device):\n",
    "    model.train()\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        for data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            data = data.to(device)\n",
    "            batch_size = data.size(0)\n",
    "            recon_data, mu, logvar = model(data)\n",
    "            loss = loss_fn(recon_data, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_size\n",
    "        avg_loss = train_loss / total_samples\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1:02}/{num_epochs}], Loss: {avg_loss:>11.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbf8799-6c46-498a-a308-f9be27e2f067",
   "metadata": {
    "tags": []
   },
   "source": [
    "![alt text](img/VAE_edit.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed1f174-3eaf-460a-9c23-8af465b4c7bc",
   "metadata": {},
   "source": [
    "The loss is defined as\n",
    "\n",
    "- $ \\begin{align} \\mathcal{L}(\\mathbf{x};\\theta,\\phi) &=  \\mathbb{E}_{\\mathbf{z} \\sim q_{\\phi} (\\mathbf{z} | \\mathbf{x})} \\left[ \\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})  \\right]- D_{KL} \\left( q_{\\phi} \\left(\\mathbf{z}|\\mathbf{x} \\right) || p (\\mathbf{z})  \\right) \\\\ &= \\text{Reconstruction Loss + Regularization} \\end{align} $\n",
    "\n",
    "The equation is simplified as\n",
    "\n",
    "- $D_{KL}(\\mathcal N_1({\\mu_1, \\sigma_1}^2)) || \\mathcal N_2({\\mu_2, \\sigma_2}^2))=\\log {\\frac{\\sigma_2}{\\sigma_1}}+\\frac{\\sigma_1^2+(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}-{1\\over 2}$\n",
    "- $D_{KL}(\\mathcal N_1({\\mu_1, \\sigma_1}^2))||\\mathcal N(0, 1))=- \\frac{1}{2}\\left(1 + 2\\log \\sigma_1- \\mu_1^2 -\\sigma_1^2   \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../dataset/i24_normalized.pt' \n",
    "batch_size = 250\n",
    "num_epochs = 50\n",
    "z_dim = 64\n",
    "lr = 0.0005\n",
    "beta1 = 0.8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset, data_loader = load_and_preprocess_data(data_path, batch_size)\n",
    "\n",
    "input_dim = dataset.shape[1]  # dataset.shape: (40000, 1, 64, 64) â†’ input_dim = 1\n",
    "\n",
    "model = VAE(z_dim, input_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, 0.98))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a1a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"VAE summary:\")\n",
    "summary(model, (input_dim, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dcd89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_vae_model(model, data_loader, optimizer, num_epochs, device)\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/vae_model.pth\")\n",
    "print(\"VAE model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff634b55-af8d-49c2-9a60-6d6ad6ca0b46",
   "metadata": {},
   "source": [
    "## 2. Testing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac540db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generated_samples(model, z_dim, device, nrows=8, ncols=8, save_path=\"img/VAE_result.png\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(nrows * ncols, z_dim).to(device)\n",
    "        samples = model.decode(z).cpu().numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(8, 8))\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            idx = i * ncols + j\n",
    "            ax[i, j].imshow(samples[idx][0, :, :], origin=\"lower\", cmap=\"viridis\")\n",
    "            ax[i, j].axis(\"off\")\n",
    "    plt.suptitle(\"Generated Samples\", fontsize=16)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=500)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb63b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use CPU in the inference, but highly recommended to use GPU\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VAE(z_dim, input_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"models/vae_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "os.makedirs(\"img\", exist_ok=True)\n",
    "visualize_generated_samples(model, z_dim, device, nrows=8, ncols=8, save_path=\"img/VAE_result.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
