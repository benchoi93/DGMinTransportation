{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614638c4-b027-437d-9159-0cc8f67309a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44007ac-16e8-43f8-bf4d-81f1112d0034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    # Set the random seed for CPU\n",
    "    torch.manual_seed(seed)\n",
    "    # Set the random seed for all GPUs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Set the random seed for numpy\n",
    "    np.random.seed(seed)\n",
    "    # Set the random seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Ensure reproducibility by disabling the benchmarking feature in PyTorch\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Example of setting the seed\n",
    "# set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a2112-262b-4e0c-a25b-16d4e2169ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working\n",
    "# Basic Convolutional Layer\n",
    "def conv3x3(in_planes, out_planes, stride=1, dilation=1, bias=False):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=bias)\n",
    "\n",
    "# Conditional Batch Normalization\n",
    "class ConditionalBatchNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.bn = nn.BatchNorm2d(num_features, affine=False)\n",
    "        self.embed = nn.Embedding(num_classes, num_features * 2 if bias else num_features)\n",
    "        self.embed.weight.data[:, :num_features].uniform_()\n",
    "        if bias:\n",
    "            self.embed.weight.data[:, num_features:].zero_()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.bn(x)\n",
    "        gamma, beta = self.embed(y).chunk(2, dim=1) if self.embed.weight.size(1) == self.num_features * 2 else (self.embed(y), None)\n",
    "        out = gamma.view(-1, self.num_features, 1, 1) * out + (beta.view(-1, self.num_features, 1, 1) if beta is not None else 0)\n",
    "        return out\n",
    "\n",
    "# Simple Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_classes, norm_layer, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, dilation=dilation)\n",
    "        self.norm1 = norm_layer(out_channels, num_classes)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels, dilation=dilation)\n",
    "        self.norm2 = norm_layer(out_channels, num_classes)\n",
    "        self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False) if in_channels != out_channels else nn.Identity()\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.act(self.norm1(self.conv1(x), y))\n",
    "        out = self.norm2(self.conv2(out), y)\n",
    "        out += self.shortcut(x)\n",
    "        return self.act(out)\n",
    "\n",
    "# Simplified Refinement Network\n",
    "class CondRefineNetDilated(nn.Module):\n",
    "    def __init__(self, device, num_classes):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.norm_layer = ConditionalBatchNorm2d\n",
    "        self.num_classes = num_classes\n",
    "        self.ngf = 64\n",
    "        \n",
    "        self.begin_conv = nn.Conv2d(4, self.ngf, kernel_size=3, padding=1)\n",
    "        self.end_conv = nn.Conv2d(self.ngf*2, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        self.res1 = ResidualBlock(self.ngf, self.ngf, num_classes, self.norm_layer)\n",
    "        self.res2 = ResidualBlock(self.ngf, 2 * self.ngf, num_classes, self.norm_layer, dilation=2)\n",
    "        self.res3 = ResidualBlock(2 * self.ngf, 2 * self.ngf, num_classes, self.norm_layer, dilation=4)\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.act(self.begin_conv(x))\n",
    "        out = self.res1(out, y)\n",
    "        out = self.res2(out, y)\n",
    "        out = self.res3(out, y)\n",
    "        out = self.act(out)\n",
    "        out = self.end_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eaff33-1fcf-4df5-bcf7-a7ac5d0d0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, device, n_steps, sigma_min, sigma_max):\n",
    "        '''\n",
    "        Score Network.\n",
    "\n",
    "        n_steps   : perturbation schedule steps (Langevin Dynamic step)\n",
    "        sigma_min : sigma min of perturbation schedule\n",
    "        sigma_min : sigma max of perturbation schedule\n",
    "\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.sigmas = torch.exp(torch.linspace(start=math.log(sigma_max), end=math.log(sigma_min), steps = n_steps)).to(device = device)\n",
    "        self.conv_layer = CondRefineNetDilated(device, n_steps) # Here is the problem.\n",
    "        self.to(device = device)\n",
    "\n",
    "    # Loss Function\n",
    "    def loss_fn(self, x, idx=None):\n",
    "        '''\n",
    "        This function performed when only training phase.\n",
    "\n",
    "        x          : real data if idx==None else perturbation data\n",
    "        idx        : if None (training phase), we perturbed random index. Else (inference phase), it is recommended that you specify.\n",
    "\n",
    "        '''\n",
    "        scores, target, sigma = self.forward(x, idx=idx, get_target=True)\n",
    "        target = target.view(target.shape[0], -1)\n",
    "        scores = scores.view(scores.shape[0], -1)        \n",
    "        losses = torch.square(scores - target).mean(dim=-1) * sigma.squeeze() ** 2\n",
    "        return losses.mean(dim=0)\n",
    "\n",
    "    # S(theta, sigma)\n",
    "    def forward(self, x, idx=None, get_target=False):\n",
    "        '''\n",
    "        x          : real data if idx==None else perturbation data\n",
    "        idx        : if None (training phase), we perturbed random index. Else (inference phase), it is recommended that you specify.\n",
    "        get_target : if True (training phase), target and sigma is returned with output (score prediction)\n",
    "\n",
    "        '''\n",
    "\n",
    "        if idx == None:\n",
    "            idx = torch.randint(0, len(self.sigmas), (x.size(0), 1)).to(device = self.device)\n",
    "            used_sigmas = self.sigmas[idx][:, :, None, None]\n",
    "            noise = torch.randn_like(x)\n",
    "            x_tilde = x + noise * used_sigmas\n",
    "            idx = idx.squeeze()\n",
    "        else:\n",
    "            idx = torch.Tensor([idx for _ in range(x.size(0))]).to(device = self.device).long()\n",
    "            x_tilde = x\n",
    "            \n",
    "        if get_target:\n",
    "            target = - 1 / (used_sigmas ) * noise \n",
    "\n",
    "            \n",
    "        output = self.conv_layer(x_tilde, idx)\n",
    "\n",
    "        return (output, target, used_sigmas) if get_target else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af6ed3-700e-4a08-bf3b-6d76136f42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnnealedLangevinDynamic():\n",
    "    def __init__(self, sigma_min, sigma_max, n_steps, annealed_step, score_fn, device, eps = 1e-1):\n",
    "        '''\n",
    "        sigma_min : minimum sigmas of perturbation schedule \n",
    "        sigma_max : maximum sigmas of perturbation schedule \n",
    "        L         : iteration step of Langevin dynamic\n",
    "        T         : annelaed step of annealed Langevin dynamic\n",
    "        score_fn  : trained score network\n",
    "        eps       : coefficient of step size\n",
    "        '''\n",
    "        self.process = torch.exp(torch.linspace(start=math.log(sigma_max), end=math.log(sigma_min), steps = n_steps))\n",
    "        self.step_size = eps * (self.process / self.process[-1] ) ** 2\n",
    "        self.score_fn = score_fn\n",
    "        self.annealed_step = annealed_step\n",
    "        self.device = device\n",
    "        \n",
    "    # One iteration of annealed step\n",
    "    def _one_annealed_step_iteration(self, x, idx):\n",
    "        '''\n",
    "        x   : perturbated data\n",
    "        idx : step of perturbation schedule\n",
    "        '''\n",
    "        self.score_fn.eval()\n",
    "        z, step_size = torch.randn_like(x).to(device = self.device), self.step_size[idx]\n",
    "        x = x + 0.5 * step_size * self.score_fn(x, idx) + torch.sqrt(step_size) * z\n",
    "        return x\n",
    "        \n",
    "    # One annealed step\n",
    "    def _one_annealed_step(self, x, idx):\n",
    "        '''\n",
    "        x   : perturbated data\n",
    "        idx : step of perturbation schedule\n",
    "        '''\n",
    "        for _ in range(self.annealed_step):\n",
    "            x = self._one_annealed_step_iteration(x, idx)\n",
    "        return x\n",
    "        \n",
    "    # One Langevin Step\n",
    "    def _one_diffusion_step(self, x):\n",
    "        '''\n",
    "        x   : sampling of prior distribution\n",
    "        '''\n",
    "        for idx in range(len(self.process)):\n",
    "            x = self._one_annealed_step(x, idx)\n",
    "            yield x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sampling(self, sampling_number, only_final=False):\n",
    "        '''\n",
    "        only_final : If True, return is an only output of final schedule step \n",
    "        '''\n",
    "        # sample = torch.rand([sampling_number, 1, 14, 14]).to(device = self.device)\n",
    "        sample = torch.rand([sampling_number, 4, 64, 64]).to(device = self.device)\n",
    "        sampling_list = []\n",
    "        \n",
    "        final = None\n",
    "        for sample in self._one_diffusion_step(sample):\n",
    "            final = sample\n",
    "            if not only_final:\n",
    "                sampling_list.append(final)\n",
    "                \n",
    "\n",
    "        return final if only_final else torch.stack(sampling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defda95e-4e1f-44eb-bbec-fd08f3303aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        \n",
    "        print('\\r' + '\\t'.join(entries), end = '')\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27847d6c-a62a-4eef-85be-22fe222968da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon of step size\n",
    "eps = 1.5e-5\n",
    "\n",
    "# sigma min and max of Langevin dynamic\n",
    "sigma_min = 0.005\n",
    "sigma_max = 10\n",
    "\n",
    "# Langevin step size and Annealed size\n",
    "n_steps = 20\n",
    "annealed_step = 50\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7affa303-415c-4ddc-9289-429745563bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(device, n_steps, sigma_min, sigma_max)\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f9fd4-3c36-4469-bbce-a866087fab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ce7d72-1e22-44e5-b4c7-350021d8440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data with min-max scaling\n",
    "\n",
    "data_min = 0 # data.min()\n",
    "data_max = data.max()\n",
    "\n",
    "data = (data - data_min) / (data_max - data_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9689b-320d-4378-9c32-3da531c4b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "\n",
    "train_data, val_data = torch.utils.data.random_split(data, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "dataiterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3fc18b-917c-43f6-bf35-2fb7e73079f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_iteration = 20000\n",
    "current_iteration = 0\n",
    "display_iteration = 1000\n",
    "sampling_number = 16\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "only_final = True\n",
    "\n",
    "losses = AverageMeter('Loss', ':.4f')\n",
    "progress = ProgressMeter(total_iteration, [losses], prefix='Iteration ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3e716-6d02-4f72-8e5a-90f6e7ed7cab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while current_iteration != total_iteration:\n",
    "    model.train()\n",
    "    try:\n",
    "        data = next(dataiterator)\n",
    "    except:\n",
    "        dataiterator = iter(train_loader)\n",
    "        data = next(dataiterator)\n",
    "    # data = data[0].to(device = device)\n",
    "    data = data.to(device = device)\n",
    "    loss = model.loss_fn(data)\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    losses.update(loss.item())\n",
    "    progress.display(current_iteration)\n",
    "    current_iteration += 1\n",
    "    \n",
    "    if current_iteration % display_iteration == 0:\n",
    "        sampling_number = 9\n",
    "        only_final = True\n",
    "        dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)\n",
    "        sample = dynamic.sampling(sampling_number, only_final)\n",
    "        \n",
    "        fig, ax = plt.subplots(3, 3, figsize=(8, 8))\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                ax[i, j].imshow(sample.cpu().numpy()[i * 3 + j][0,:,:], origin=\"lower\")\n",
    "                ax[i, j].axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b11c732-16bf-49bc-9416-2c4779d0957e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eps_list = [7.5e-4, 5e-4, 2.5e-4, 1e-4, 7.5e-5]\n",
    "sigma_min_list = [0.05]\n",
    "sigma_max_list = [1]\n",
    "\n",
    "# Define the testing function\n",
    "for eps in eps_list:\n",
    "    for sigma_min in sigma_min_list:\n",
    "        for sigma_max in sigma_max_list:\n",
    "            print(f\"Testing with eps={eps}, sigma_min={sigma_min}, sigma_max={sigma_max}\")\n",
    "\n",
    "            # Langevin step size and Annealed size\n",
    "            n_steps = 20\n",
    "            annealed_step = 50\n",
    "\n",
    "            device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "            model = Model(device, n_steps, sigma_min, sigma_max)\n",
    "            optim = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "            dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)\n",
    "\n",
    "            total_iteration = 5000\n",
    "            current_iteration = 0\n",
    "            display_iteration = 500\n",
    "            sampling_number = 16\n",
    "            device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            only_final = True\n",
    "\n",
    "            losses = AverageMeter('Loss', ':.4f')\n",
    "            progress = ProgressMeter(total_iteration, [losses], prefix='Iteration ')\n",
    "\n",
    "            while current_iteration != total_iteration:\n",
    "                model.train()\n",
    "                try:\n",
    "                    data = next(dataiterator)\n",
    "                except:\n",
    "                    dataiterator = iter(train_loader)\n",
    "                    data = next(dataiterator)\n",
    "                # data = data[0].to(device = device)\n",
    "                data = data.to(device = device)\n",
    "                loss = model.loss_fn(data)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                losses.update(loss.item())\n",
    "                progress.display(current_iteration)\n",
    "                current_iteration += 1\n",
    "\n",
    "                if current_iteration % display_iteration == 0:\n",
    "                    sampling_number = 9\n",
    "                    only_final = True\n",
    "                    dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)\n",
    "                    sample = dynamic.sampling(sampling_number, only_final)\n",
    "\n",
    "                    fig, ax = plt.subplots(3, 3, figsize=(8, 8))\n",
    "                    for i in range(3):\n",
    "                        for j in range(3):\n",
    "                            ax[i, j].imshow(sample.cpu().numpy()[i * 3 + j][0,:,:], origin=\"lower\")\n",
    "                            ax[i, j].axis(\"off\")\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5908b8a-625e-4429-93b3-97333d0da0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_list = [1.5e-4, 5e-5, 1.5e-5, 1.5e-6]\n",
    "sigma_min_list = [0.05, 0.01, 0.005, 0.0005]\n",
    "sigma_max_list = [50, 25, 10, 1]\n",
    "\n",
    "# Define the testing function\n",
    "for eps in eps_list:\n",
    "    for sigma_min in sigma_min_list:\n",
    "        for sigma_max in sigma_max_list:\n",
    "            print(f\"Testing with eps={eps}, sigma_min={sigma_min}, sigma_max={sigma_max}\")\n",
    "\n",
    "            # Langevin step size and Annealed size\n",
    "            n_steps = 20\n",
    "            annealed_step = 50\n",
    "\n",
    "            device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "            model = Model(device, n_steps, sigma_min, sigma_max)\n",
    "            optim = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
    "            dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)\n",
    "\n",
    "            total_iteration = 20000\n",
    "            current_iteration = 0\n",
    "            display_iteration = 1000\n",
    "            sampling_number = 16\n",
    "            device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "            only_final = True\n",
    "\n",
    "            losses = AverageMeter('Loss', ':.4f')\n",
    "            progress = ProgressMeter(total_iteration, [losses], prefix='Iteration ')\n",
    "\n",
    "            while current_iteration != total_iteration:\n",
    "                model.train()\n",
    "                try:\n",
    "                    data = next(dataiterator)\n",
    "                except:\n",
    "                    dataiterator = iter(train_loader)\n",
    "                    data = next(dataiterator)\n",
    "                # data = data[0].to(device = device)\n",
    "                data = data.to(device = device)\n",
    "                loss = model.loss_fn(data)\n",
    "\n",
    "                optim.zero_grad()\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                losses.update(loss.item())\n",
    "                progress.display(current_iteration)\n",
    "                current_iteration += 1\n",
    "\n",
    "                if current_iteration % display_iteration == 0:\n",
    "                    sampling_number = 9\n",
    "                    only_final = True\n",
    "                    dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)\n",
    "                    sample = dynamic.sampling(sampling_number, only_final)\n",
    "\n",
    "                    fig, ax = plt.subplots(3, 3, figsize=(8, 8))\n",
    "                    for i in range(3):\n",
    "                        for j in range(3):\n",
    "                            ax[i, j].imshow(sample.cpu().numpy()[i * 3 + j][0,:,:], origin=\"lower\")\n",
    "                            ax[i, j].axis(\"off\")\n",
    "                    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e072e56-11f2-44e9-91da-5d163ae87c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_number = 4\n",
    "only_final = True\n",
    "n_steps = 25\n",
    "annealed_step = 200\n",
    "dynamic = AnnealedLangevinDynamic(sigma_min, sigma_max, n_steps, annealed_step, model, device, eps=eps)\n",
    "sample = dynamic.sampling(sampling_number, only_final)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8, 8))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax[i, j].imshow(sample.cpu().numpy()[i * 2 + j][0,:,:], origin=\"lower\")\n",
    "        ax[i, j].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
